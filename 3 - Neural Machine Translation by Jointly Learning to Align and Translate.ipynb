{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keon/seq2seq/\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/multi30k/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Multi30k.download('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>')\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = TranslationDataset.splits(      \n",
    "  path = 'data/multi30k',  \n",
    "  exts = ['.de', '.en'],   \n",
    "  fields = [('src', SRC), ('trg', TRG)],\n",
    "  train = 'train', \n",
    "  validation = 'val', \n",
    "  test = 'test2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train.src, min_freq=2)\n",
    "TRG.build_vocab(train.trg, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test), batch_size=BATCH_SIZE, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://discuss.pytorch.org/t/how-can-i-know-which-part-of-h-n-of-bidirectional-rnn-is-for-backward-process/3883/4\n",
    "- https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "- https://discuss.pytorch.org/t/get-forward-and-backward-output-seperately-from-bidirectional-rnn/2523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, hidden=None):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "                \n",
    "        #outputs = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #outputs[-1,  :, :self.hid_dim] is the last of the forwards RNN\n",
    "        #outputs[ 0,  :, self.hid_dim:] is the last of the backwards RNN\n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #therefore:\n",
    "        assert (torch.cat((outputs[-1,:,:self.hid_dim], outputs[0,:,self.hid_dim:]),dim=1) == torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim=1)).all()\n",
    "        \n",
    "        #we sum, but can take the mean or pass through a linear layer\n",
    "        outputs = outputs[:, :, :self.hid_dim] + outputs[:, :, self.hid_dim:]\n",
    "        hidden = hidden[-2,:,:] + hidden[-1,:,:]\n",
    "        \n",
    "        #outputs = [sent len, batch size, hid dim]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "        \n",
    "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def attention(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, hid dim]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        energy = self.attn(torch.cat((hidden, encoder_outputs),dim=2))\n",
    "        \n",
    "        #energy = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, hid dim, src sent len]\n",
    "        \n",
    "        #v = [hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, hid_dim]\n",
    "                \n",
    "        energy = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #energy = [batch size, src len]\n",
    "        \n",
    "        return F.softmax(energy, dim=1).unsqueeze(1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [bsz]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, bsz]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, bsz, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        #a = [bsz, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [bsz, src sent len, hid dim]\n",
    "        \n",
    "        context = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #context = [bsz, 1, hid dim]\n",
    "        \n",
    "        context = context.permute(1, 0, 2)\n",
    "        \n",
    "        #context = [1, bsz, hid dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        \n",
    "        #rnn_input = [1, bsz, hid dim + emb dim]\n",
    "              \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #o = [1, bsz, hid dim]\n",
    "        #h = [n layers, bsz, hid dim]\n",
    "        \n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, context), dim=1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT, BIDIRECTIONAL)\n",
    "dec = Decoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        #trg = [sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards summed together\n",
    "        #hidden is the final hidden state of the input sequence, back and forwards summed together\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        src,trg=src.to(device),trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            src,trg=src.to(device),trg.to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 4.660           | Train PPL: 105.628 | Val. Loss: 4.509           | Val. PPL:  90.802 | time:105.396\n",
      "| Epoch: 02 | Train Loss: 3.757           | Train PPL:  42.801 | Val. Loss: 4.079           | Val. PPL:  59.076 | time:104.761\n",
      "| Epoch: 03 | Train Loss: 3.364           | Train PPL:  28.895 | Val. Loss: 3.913           | Val. PPL:  50.061 | time:104.678\n",
      "| Epoch: 04 | Train Loss: 3.061           | Train PPL:  21.359 | Val. Loss: 3.784           | Val. PPL:  44.005 | time:104.601\n",
      "| Epoch: 05 | Train Loss: 2.827           | Train PPL:  16.901 | Val. Loss: 3.719           | Val. PPL:  41.216 | time:104.718\n",
      "| Epoch: 06 | Train Loss: 2.650           | Train PPL:  14.158 | Val. Loss: 3.689           | Val. PPL:  39.994 | time:104.564\n",
      "| Epoch: 07 | Train Loss: 2.486           | Train PPL:  12.012 | Val. Loss: 3.658           | Val. PPL:  38.782 | time:105.775\n",
      "| Epoch: 08 | Train Loss: 2.360           | Train PPL:  10.589 | Val. Loss: 3.700           | Val. PPL:  40.447 | time:104.441\n",
      "| Epoch: 09 | Train Loss: 2.234           | Train PPL:   9.339 | Val. Loss: 3.716           | Val. PPL:  41.105 | time:104.408\n",
      "| Epoch: 10 | Train Loss: 2.144           | Train PPL:   8.533 | Val. Loss: 3.737           | Val. PPL:  41.967 | time:104.661\n",
      "| Epoch: 11 | Train Loss: 2.061           | Train PPL:   7.851 | Val. Loss: 3.702           | Val. PPL:  40.534 | time:104.821\n",
      "| Epoch: 12 | Train Loss: 1.979           | Train PPL:   7.236 | Val. Loss: 3.750           | Val. PPL:  42.535 | time:103.074\n",
      "| Epoch: 13 | Train Loss: 1.912           | Train PPL:   6.769 | Val. Loss: 3.789           | Val. PPL:  44.209 | time:96.688\n",
      "| Epoch: 14 | Train Loss: 1.857           | Train PPL:   6.407 | Val. Loss: 3.812           | Val. PPL:  45.256 | time:97.505\n",
      "| Epoch: 15 | Train Loss: 1.804           | Train PPL:   6.073 | Val. Loss: 3.847           | Val. PPL:  46.851 | time:99.446\n",
      "| Epoch: 16 | Train Loss: 1.750           | Train PPL:   5.756 | Val. Loss: 3.850           | Val. PPL:  47.003 | time:99.498\n",
      "| Epoch: 17 | Train Loss: 1.719           | Train PPL:   5.581 | Val. Loss: 3.908           | Val. PPL:  49.811 | time:99.852\n",
      "| Epoch: 18 | Train Loss: 1.660           | Train PPL:   5.259 | Val. Loss: 3.939           | Val. PPL:  51.363 | time:99.461\n",
      "| Epoch: 19 | Train Loss: 1.633           | Train PPL:   5.121 | Val. Loss: 3.906           | Val. PPL:  49.698 | time:98.414\n",
      "| Epoch: 20 | Train Loss: 1.585           | Train PPL:   4.877 | Val. Loss: 3.970           | Val. PPL:  52.986 | time:98.885\n",
      "| Epoch: 21 | Train Loss: 1.590           | Train PPL:   4.904 | Val. Loss: 3.943           | Val. PPL:  51.565 | time:98.364\n",
      "| Epoch: 22 | Train Loss: 1.542           | Train PPL:   4.675 | Val. Loss: 4.031           | Val. PPL:  56.330 | time:100.205\n",
      "| Epoch: 23 | Train Loss: 1.514           | Train PPL:   4.544 | Val. Loss: 4.000           | Val. PPL:  54.594 | time:101.230\n",
      "| Epoch: 24 | Train Loss: 1.490           | Train PPL:   4.439 | Val. Loss: 4.029           | Val. PPL:  56.216 | time:99.847\n",
      "| Epoch: 25 | Train Loss: 1.471           | Train PPL:   4.353 | Val. Loss: 4.063           | Val. PPL:  58.166 | time:98.369\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "N_EPOCHS = 25\n",
    "CLIP = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir('.save'):\n",
    "    os.makedirs('.save')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    tic=time.time()\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '.save/tut3_model.pt')\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} \\\n",
    "          | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} \\\n",
    "          | Val. PPL: {math.exp(valid_loss):7.3f} | time:{(time.time()-tic):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.635 | Test PPL:  37.898\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('.save/tut3_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
